<html>
    <head>
        <title>Intro-to-Digital-Audio</title>
        <link rel="stylesheet" type="text/css" href="css/styling.css">
    </head>
    <body>
       <div id="header">
       <h1> An Introduction</h1>
       </div>
       <div id="header2">
       <h1> to Digital Audio </h1>
       </div>
       <div id="content">
       <h2>What is audio? Moreover, what is sound? These are important questions to ask and answer if you study music.</h2>

       <p><b>Music</b> is often defined as ordered <b>sound</b>, sound being perceivable changes in air pressure.</p>

        <p>When these changes occur with <a href="https://en.wikipedia.org/wiki/Periodic_function">periodicity</a> 
        at rates between 20 and 20,000 cycles per second (Hertz/Hz), they are perceived as <b>pitch</b>.</p> 
        
        <p>When they occur slower, they are perceived as <b>rhythm</b>, and are imperceivable when faster than ~20kHz.</p>

        <p><b>Audio</b> is an abstraction of sound.</p>
        
        <p>Where sound describes changes in air pressure over time, 
        audio describes the change of an arbitrary value (known as <b>amplitude</b>) over time, 
        particularly changes in the <b>audio band</b> of 20Hz-20kHz.</p>

        <p>Audio allows for high-resolution representation of music in mediums other than sound, 
        which allows for the <b>recording</b> and <b>playback</b> of music.</p> 

        <figure>
        <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/8/83/Brownwaxcylinders.jpg/440px-Brownwaxcylinders.jpg" />
        <figcaption>
        Wax cylinders, an early recording medium 
        </figcaption>
        </figure>

        <p><a href="https://en.wikipedia.org/wiki/Electrification">Electrification</a> 
        in the 20th century allowed for the representation of audio signals as changes of voltage in electric current, 
        drastically changing many aspects of music. 
        This included the invention of electric and electronic instruments, 
        sound amplification, and effect processing.</p> 

        <p>Since the dawn of the 21st century, 
        <a href="https://en.wikipedia.org/wiki/Computer">digital computers</a> 
        have become the standard tool for composition, recording, processing, and playback- 
        <b>nearly all music is touched by computers in some way.</b></p>

        <p>Despite this, curriculum in digital audio is underdeveloped in many university and high school music programs.</p>

        <p><b>The goal of this document</b> is to confer a basic understanding of the underlying theory behind digital audio, 
        as well as an understanding of the tools the medium makes available.</p> 

        <h2> Digital Audio Basics </h2>

       <h3> Sampling </h3>

        <p>The underlying theory of digital audio was developed in tandem with the digital computer itself.</p>

        <p>The fundamental hurdle was how to represent <a href="https://en.wikipedia.org/wiki/Continuous_function">continuous</a> data 
        as <a href="https://en.wikipedia.org/wiki/Discrete_mathematics">discrete</a> data, 
        the only kind a digital computer can operate on. 
        The answer was found in <a href="https://en.wikipedia.org/wiki/Sampling_(signal_processing)">sampling</a>.</p>

        <p>You may know that the illusion of continuous motion in a film is created by the rapid playback of still images known as <i>frames</i>. 
        Audio sampling operates on the same principal.</p>

        <p>A major difference is that, while frame rates for video are set between 24-60 per second, 
        audio <i>sample rates</i> are typically set above <b>40,000</b> samples per second.</p> 

        <figure>
        <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Signal_Sampling.svg/600px-Signal_Sampling.svg.png" />
        <figcaption>
        A visual depiction of sampling
        </figcaption>
        </figure>

        <p>The reason for this high framerate is explained by the <a href="https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem">Nyquist-Shannon sampling theorem</a>, 
        which posits that, to accurately sample a signal, the sampling rate must be at least twice the signal's highest frequency component.</p> 
        
        <p>Because humans can hear up to ~20,000 Hertz, only sample rates greater than ~40,000 are considered sufficient.</p>

        <h3> Time vs. Frequency Domain Representation </h3>

        <p>Digital audio can be conceptualized and operated on in two distinct ways.</p>

        <p>One is the <b>time domain</b>, which deals with samples as a sequence of discrete <b>amplitude</b> values.</p>

        <p>Time domain audio data maps well to a two-dimensional graphic representation:<br>
        for each sample or <i>timestep</i> (typically x-axis), there is a corresponding amplitude (y-axis).</p>

        <figure>
        <img src="https://www.electronicshub.org/wp-content/uploads/2015/07/11.jpg"/>
        <figcaption>
        A time domain representation of a sine wave, the most elemental waveform
        </figcaption>
        </figure>

        <figure>
        <img src="https://bluskysoftware.com/audacity/manual/m/images/5/55/stereo_track_example.png"/>
        <figcaption>
        A time domain representation of a song's left and right channels
        </figcaption>
        </figure>

        <p>Time domain representations don't always track well to human perception of audio however, 
        because we do not perceive amplitude or <b>phase</b> but instead its derivate, <b>frequency</b>.</p>

        <p>Thankfully, frequency data can be extracted from time domain data using the <b>fourier transform</b>, 
        one of the most important algorithms in signal processing.</p>

        <p>In the 18th century, mathematician <a href="https://en.wikipedia.org/wiki/Joseph_Fourier">Joseph Fourier</a> 
        proposed that any signal can be represented as a series of sinusoidal frequencies summed in different proportions.
        By computing the fourier transform and graphing time on the x-axis, 
        frequency on the y-axis, 
        and the amplitude of each frequency component as color, 
        a <b>frequency domain</b> representation called a <b>spectrogram</b> can be created.</p>

        <!--TODO: Spectrogram vs other frequency domain representations-->

        <figure>
        <img src="https://manual.audacityteam.org/m/images/6/68/spectrogramview_10a.png"/>
        <figcaption>
        A spectrogram of a song showing how different instruments affect its frequency content
        </figcaption>
        </figure>

        <h3> Basic Operations </h3> 

        <p>When working with digital audio data, 
        the two most fundamental operations are <b>addition</b> and <b>multiplication</b>.</p>

        <p>Adding two signals is equivalent to <a href="https://en.wikipedia.org/wiki/Audio_mixing_(recorded_music)">mixing</a> 
        in the analog domain. To create a waveform that represents a song, 
        the individual instrument tracks are added together.</p>

        <p>Addition is also the basis of digital 
        <a href="https://en.wikipedia.org/wiki/Electronic_oscillator">oscillators</a>,
        an essential building block of audio synthesis. 
        A digital oscillator adds to a phase value at each timestep, 
        and resets it when a specified maximum value is reached.</p>

        <p> Multiplication scales the volume of samples. 
        Multiplying every sample of an audio signal by a positive number less than 1 results in 
        <a href="https://en.wikipedia.org/wiki/Attenuation">attenuation</a>, 
        essentially the reduction of a signal's volume. 
        Multiplying a signal's amplitude values by a number greater than 1 represents 
        <a href="https://en.wikipedia.org/wiki/Amplifier">amplification</a>, 
        an increase in volume.</p>

        <figure>
        <img src="https://media.geeksforgeeks.org/wp-content/uploads/20231116161709/Attenuation-in-a-signal-660.png"/>
        <figcaption>
        A visual depiction of attenuation 
        </figcaption>
        </figure>

        <figure>
        <img src="https://www.eeeguide.com/wp-content/uploads/2022/11/Class-A-Power-Amplifiers-Direct-Coupled-with-Resistive-Load-18.jpg"/>
        <figcaption>
        A visual depiction of amplification 
        </figcaption>
        </figure>

        <p>As shown in the effects section, 
        more complicated transformations are made by chaining together many simple addition 
        and multiplication operations.</p>

        <h3> Synthesis </h3> 

        <!--TODO: Add Images-->

        <p>Much early work in digital audio revolved not around the processing of recorded musical signals, 
        but instead the synthesis of sounds from scratch. <b>Digital synthesis</b> remains an active field of research, 
        as the possibilities it affords are nearly limitless.</p>

        <p>Working from the principles of fourier analysis, 
        <a href="https://en.wikipedia.org/wiki/Additive_synthesis">additive synthesis</a> creates timbres through the addition of sine waves, 
        and was one of the earliest used techniques.</p>

        <p>Because additive synthesis requires an oscillator for every frequency component, 
        it is considered a computationally expensive technique that poses problems in the context of limited computing resources.</p>

        <p><a href="https://en.wikipedia.org/wiki/Frequency_modulation_synthesis">Frequency modulation synthesis</a> 
        was developed as a computationally-inexpensive alternative to additive synthesis. 
        It allows for the creation of many frequencies from as little as two oscillators, 
        but is less intuitive to parameterize than additive synthesis.</p>

        <p>Less prevalent but equally interesting techniques include <a href="https://en.wikipedia.org/wiki/Physical_modelling_synthesis">physical modeling</a>, 
        <a href="https://en.wikipedia.org/wiki/Subtractive_synthesis">subtractive</a>, 
        and <a href="https://en.wikipedia.org/wiki/Granular_synthesis">granular</a> synthesis.</p>

        <h3> Signal Processing </h3>

        <p>As mentioned prior, digital audio allows for the manipulation of existing musical signals.<br>
        Operations range from basic attenuation to radical alterations such as <a href="https://en.wikipedia.org/wiki/Auto-Tune">pitch correction</a>.</p>
        
        
       <p> When <a href="https://en.wikipedia.org/wiki/Digital_signal_processing">digital signal processing</a> calculations can occur faster than human perception of discrete events in time, 
        perceptually <a href="https://en.wikipedia.org/wiki/Real-time_computing">real-time</a> processing is possible.</p> 

        <p>Modern computers are often capable of performing billions of operations per seconds, 
        making real-time digital audio processing increasingly popular among practicing musicians.</p>

        <h2> A Survey of Digital Audio Tools </h2>
        <!--Intro Paragraph-->

        <h3> Effects </h3>
        
        <h4> <a href="https://en.wikipedia.org/wiki/Equalization_(audio)">Equalization</a> </h4>

        <p>refers to the attenuation of certain frequencies in an audio signal, 
        and is one of the most ubiquitous audio effects.<br>
        Modern EQ is performed with <a href="https://en.wikipedia.org/wiki/Digital_filter">digital filters</a> 
        that track amplitude over time and respond according to user-friendly parameters.</p>

        <p>Initially, filters were used to compensate for the infidelity of early recording techniques, 
        but have since become powerful tools for both sweetening natural timbres and sculpting wholly new electronic sounds.</p>
        
        <h4> <a href="https://en.wikipedia.org/wiki/Dynamic_range_compression">Dynamic range compression</a> </h4>

        <p>is one of the most widely used effects in audio.<br>
        By lowering the volume of loud sections on a track, 
        the entire track's volume can be raised without fear of <a href="https://en.wikipedia.org/wiki/Clipping_(audio)">clipping</a>.
        This allows for increased perceptual loudness, and creates consistency in a clip's volume.</p>

        <p>While compression is an essential tool in popular music, its overuse has created controversy in the music industry, 
        culminating in the <a href="https://en.wikipedia.org/wiki/Loudness_war#2000s">loudness war</a> of the early 2000s.</p>

        <p>It is generally recommended to use compression subtly to sweeten sounds, 
        rather than as an effect to transform their character.</p>

        <figure>
        <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/b0/Metallica_My_Apocalypse_waveform.png/1596px-Metallica_My_Apocalypse_waveform.png" />
        <figcaption>
        Two mixes of <i>My Apocalypse</i> by Metallica, one heavily compressed (top)
        </figcaption>
        </figure>

        <h4> Time Effects, </h4> 

        <p>like digital filters, leverage memory of a signal's past values to create output samples.<br> 
        Digital audio is optimal for this, because it is capable of nearly perfect recording, storage and playback of audio samples.
        The first commercial digital audio devices were in fact time effects, made by <a href="https://en.wikipedia.org/wiki/Eventide,_Inc">Eventide Inc.</a> in the 1970s.</p> 
        
        <p><a href="https://en.wikipedia.org/wiki/Reverb_effect">Reverb</a> is a ubiquitous time effect that mimics the physical phenomenon of acoustic reflection, 
        where audio in a room is not only heard coming from its source but also reflected from many surfaces.</p>

        <p>Another popular time effect is <a href="https://en.wikipedia.org/wiki/Delay_(audio_effect)">delay</a>, 
        which repeats signal history preceding the present by an amount of time perceived as a discrete repeat.</p>

        <h4> Modulation Effects </h4>

        <p>involve the manipulation of an audio signal by a periodic function, 
        typically an elemental waveform produced by a 
        <b>low-frequency oscillator</b> (<a href="https://en.wikipedia.org/wiki/Electronic_oscillator">LFO</a>).<br>
        When one signal modulates another, the signal being modified is called the <b>carrier</b> and the signal that modifies it is called the <b>modulator</b>.</p>
        
        <p>The simplest modulation effect is tremolo, which modulates a signal's volume. 
        More complex modulation effects include chorus, detune and phaser.</p>
        <!--Explain modulation effects with greater depth (and/or add Wiki links)-->

        <h4> Spectral Processing </h4>

        <p>involves the manipulation of audio data in the frequency domain.<br>
        Common examples are <a href="https://en.wikipedia.org/wiki/Convolution">convolution</a> and 
        <a href="https://en.wikipedia.org/wiki/Pitch_correction">pitch-correction</a>,
        effects that are difficult to execute cleanly in the time domain.</p>

        <h3> Composition and Production </h3> 

        <p>In the 21st century, <a href="https://en.wikipedia.org/wiki/Music_streaming_service">streaming</a> 
        has become the dominant way recorded music is consumed, 
        accounting for the majority of revenue in the US music industry since 2016.<br>
        Streaming necessitates the <b>distribution</b> and <b>playback</b> of music in digital formats, 
        which is conducive to its <b>production</b> in the digital domain as well.</p>

        <p>Digital music production is typically accomplished in the context of a 
        <b>Digital Audio Workstation</b> (<a href="https://en.wikipedia.org/wiki/Digital_audio_workstation">DAW</a>),
        a tool that encapsulates the functions of a <a href="https://en.wikipedia.org/wiki/Recording_studio">recording studio</a> 
        in highly portable software.</p>

        <figure>
        <img src="https://upload.wikimedia.org/wikipedia/en/4/47/Protools9screen.png" />
        <figcaption>
        The graphical user interface of <i>Pro Tools</i>, a popular DAW
        </figcaption>
        </figure>

        <p>DAWs allow for almost any digital signal processing imaginable, 
        because they generally support <a href="https://en.wikipedia.org/wiki/Audio_plug-in">plugins</a>, 
        external software meant to run inside large applications.</p>

        <p>Another tool is <a href="https://en.wikipedia.org/wiki/Scorewriter">music notation software</a>, 
        which digitizes composition in 
        <a href="https://en.wikipedia.org/wiki/Musical_notation#Modern_staff_notation">staff notation</a>.<br> 
        These programs allow for real-time playback of music as it is being written, 
        and generally accept input in the form of keyboard, mouse, and MIDI data.</p>

        <figure>
        <img src="https://upload.wikimedia.org/wikipedia/commons/0/01/Musescore_4_Screenshot.png" />
        <figcaption>
        <i>MuseScore</i>, 
        a <a href="https://en.wikipedia.org/wiki/Free_and_open-source_software">free & open-source</a> 
        music notation application 
        </figcaption>
        </figure>

        <!--TODO: introduce Audio Repair Tools and MIDI-->

        <h3> Analysis Tools </h3> 

       <p>Many digital audio tools involve <a href="https://en.wikipedia.org/wiki/Music_visualization">visualization</a>
        of some kind. Usually these are practical tools for measuring aspects of an audio signal like volume, 
        frequency content or <a href="https://en.wikipedia.org/wiki/Spatial_music">spatial image</a>, 
        but sometimes audio visualization can be an artistic representation used to augment performance or playback.</p>

        <p>Applications for designing artistic visualizations include 
        <a href="https://en.wikipedia.org/wiki/TouchDesigner">TouchDesigner</a>,
        <a href="https://cycling74.com/products/jitter">Jitter</a>, 
        and <a href="https://en.wikipedia.org/wiki/MilkDrop">MilkDrop</a>. 
        Their increasing prevalence in EDM performance has resulted in the growth of
        a community of 'VJs' or 'Visual Jockeys.'</p>

        <p><a href="https://en.wikipedia.org/wiki/Music_information_retrieval">Music information retrieval</a>
        is a subfield of <a href="https://en.wikipedia.org/wiki/Computer_audition">computer audition</a>
        that studies how audio data maps to music cognition. It informs the generation of music using 
        <a href="https://en.wikipedia.org/wiki/Music_and_artificial_intelligence">artificial intelligence</a>, 
        and allows for categorization of digital audio by markers such as tempo, key, mood, and even genre.</p>
        
        <p>By mapping the conventions of a music system such as 
        <a href="https://en.wikipedia.org/wiki/Tonality">tonal harmony</a> to audio data, 
        music conforming to that system can be generated <a href="https://en.wikipedia.org/wiki/Algorithm">algorithmically</a>.</p>

        <h2> Conclusion </h2> 
        <p>While digital audio is a field of such breadth and depth that it takes a lifetime to master,
        I hope that this brief document can at least provide a solid enough introduction 
        to have readers asking the right questions.</p>

        <p>If you are interested in learning more about digital audio, 
        the following resources are recommended as next steps:</p>

        <p>
        <a href="https://msp.ucsd.edu/techniques.htm">The Theory and Technique of Electronic Music - Miller Puckette</a>
        </p>

        <p>
        <a href="https://doi.org/10.2307/3680137">An Introduction to the Mathematics of Digital Signal Processing: 
        Part I: Algebra, Trigonometry, and the Most Beautiful Formula in Mathematics - F. R. Moore</a>
        </p>

        <p>
        <a href="https://itconnect.uw.edu/tools-services-support/teaching-learning/workshops/online-tutorials/digital-audio-online-curriculum/">
        UW Digital Audio Online Tutorials</a> 
        </p>
        <p>
        <a href="https://music.arts.uci.edu/dobrian/maxcookbook/">UCI Max Cookbook</a>
        </p>

        <p>
        <a href="https://www.youtube.com/@TheAudioProgrammer">The Audio Programmer YouTube Channel</a>
        </p>

        </div>
    </body>
</html>